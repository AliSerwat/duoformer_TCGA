{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a03ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_containing_this_file = Path(__file__).resolve().parent\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb92a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, dir_containing_this_file)\n",
    "import torch\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from timm.models.vision_transformer import init_weights_vit_timm, get_init_weights_vit\n",
    "from timm.models._manipulate import named_apply\n",
    "from timm.layers import trunc_normal_\n",
    "import timm\n",
    "from multiscale_attn import *\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b07673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiscaleTransformer(VisionTransformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained=False,\n",
    "        depth=12,\n",
    "        scales=2,\n",
    "        num_heads=6,\n",
    "        patch_size=16,\n",
    "        embed_dim=384,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_norm=False,\n",
    "        drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        norm_layer=None,\n",
    "        act_layer=None,\n",
    "        init_values=1e-5,\n",
    "        num_classes=1000,\n",
    "        model_type=\"scaleformer\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            depth=depth,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "        )  # class_token=False,global_pool=''\n",
    "        pos_drop_rate = 0.0\n",
    "        patch_drop_rate = 0.0  # patch dropout in vision transformer's implementation. Here we do not use it for scale attention.\n",
    "\n",
    "        self.dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "        self.norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.act_layer = act_layer or nn.GELU\n",
    "        self.embed_dim = embed_dim\n",
    "        self.model = model_type\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                MultiscaleBlock(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_norm=qk_norm,\n",
    "                    init_values=init_values,\n",
    "                    proj_drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=self.dpr[i],\n",
    "                    norm_layer=self.norm_layer,\n",
    "                    act_layer=self.act_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.num_patches = self.patch_embed.num_patches\n",
    "        # with patch token\n",
    "        # self.cls_token1 = nn.Parameter(torch.randn(1, 1, 1, embed_dim))\n",
    "        if scales == 2:\n",
    "            self.fea_dim = 6  # 6\n",
    "        elif scales == 3:\n",
    "            self.fea_dim = 22\n",
    "        elif scales == 4:\n",
    "            self.fea_dim = 86  # 2,6,22,86\n",
    "        # without patch token\n",
    "        self.cls_token1 = None\n",
    "        # self.fea_dim = 5 # 1,5,21,85\n",
    "        self.pos_embed_for_scale = nn.Parameter(\n",
    "            torch.randn(1, 1, self.fea_dim, embed_dim)\n",
    "        )\n",
    "        self.pos_drop_for_scale = nn.Dropout(p=pos_drop_rate)\n",
    "        self._init_weights()\n",
    "\n",
    "        # if pretrained,load half of the attention weights\n",
    "        # if pretrained:\n",
    "        # vanilla_model = torchvision.models.vit_base_patch16_224_in21k()\n",
    "        # https://console.cloud.google.com/storage/browser/_details/vit_models/imagenet21k/R50%2BViT-B_16.npz;tab=live_object\n",
    "        # https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_32.npz\n",
    "        # vanilla_model= timm.models.vision_transformer.vit_base_patch32_224(pretrained=True,num_classes = num_classes)\n",
    "        # pretrained_state_dict = vanilla_model.state_dict()\n",
    "        # for i in range(12):\n",
    "        #     self.blocks[i].load_state_dict(vanilla_model.blocks[i].state_dict())\n",
    "\n",
    "    def _init_weights(self):\n",
    "        super()._init_weights(\n",
    "            self.pos_embed_for_scale\n",
    "        )  # give a dummy input to the parent function in timm\n",
    "        # my additional modifications here\n",
    "        trunc_normal_(self.pos_embed_for_scale, std=0.036)  # 0.036,0.02\n",
    "        if self.cls_token1 is not None:\n",
    "            nn.init.normal_(self.cls_token1, std=0.036)  # 0.036,1e-6\n",
    "        named_apply(get_init_weights_vit(mode=\"\"), self.blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.patch_embed(x) # [N, 14*14, 384]\n",
    "        # if len(x) == 1:\n",
    "        #     B,D,H,W = x['3'].shape\n",
    "        #     if (H,W) == (7,7): # only layer 4 used\n",
    "        #         x = x['3'].reshape(B, D,49).permute(0,2,1) # flatten 7x7 to 49\n",
    "        #         cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        #         x = torch.cat((cls_token,x),dim=1)\n",
    "        #         x = self.pos_drop(x+self.pos_embed) # for a single layer of feature\n",
    "        # elif len(x) == 2:\n",
    "        #     B = x['2'].shape[0]\n",
    "        #     if self.model == 'vit':\n",
    "        #         x['2'] = x['2'].reshape(B,self.embed_dim ,-1)\n",
    "        #         x['3'] = x['3'].reshape(B,self.embed_dim ,-1)\n",
    "        #         x = torch.cat((x['3'],x['2']),dim=2).permute(0,2,1)\n",
    "        #         cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        #         x = torch.cat((cls_token,x),dim=1)\n",
    "        #         x = self.pos_drop(x+self.pos_embed1)\n",
    "\n",
    "        if (\n",
    "            self.model == \"scaleformer\"\n",
    "        ):  # features already processed, [B, 49, 1/5/21/85, d]\n",
    "            B = x.shape[0]\n",
    "            cls_token = self.cls_token.expand(\n",
    "                B, -1, -1, -1\n",
    "            )  # Pretrained CLS token: 1,1,d -> N,1,1,d\n",
    "            # if with patch token\n",
    "            # x = torch.cat((self.cls_token1.expand(B, self.num_patches, -1, -1), x), dim=2) #  N,49,1+85,d\n",
    "            x = self.pos_drop_for_scale(\n",
    "                x + self.pos_embed_for_scale\n",
    "            )  # [N, num_patches+cls_token ,channels] N,196+1,d\n",
    "            # if without patch token\n",
    "            # x = self.pos_drop(x + self.pos_embed_for_scale) # [N, 49 , 85, channels]\n",
    "            for i in range(0, len(self.blocks)):\n",
    "                x = self.blocks[i].forward_change_order_attn1(x)\n",
    "            # cls_token = x[:,:,0,:]\n",
    "            # return cls_token\n",
    "            for i in range(0, len(self.blocks)):\n",
    "                if i == 0:\n",
    "                    # x,cls_token = self.blocks[i].forward_new_block1(x,cls_token,self.pos_embed,self.pos_drop)\n",
    "                    x = self.blocks[i].forward_change_order_attn2_block1(\n",
    "                        x, cls_token, self.pos_embed, self.pos_drop\n",
    "                    )\n",
    "                else:\n",
    "                    cls_token = self.blocks[i].forward_change_order_attn2(x)\n",
    "                    # x,cls_token = self.blocks[i].forward_new(x,cls_token)\n",
    "\n",
    "            cls_token = self.norm(cls_token)\n",
    "\n",
    "        # if self.model == 'vit':\n",
    "        #     for i in range(0, len(self.blocks)):\n",
    "        #         x = self.blocks[i](x)\n",
    "        #     x = self.norm(x)\n",
    "        #     cls_token = x[:, 0]\n",
    "\n",
    "        return self.head(\n",
    "            cls_token\n",
    "        ).squeeze()  # Assuming CLS carries information of patches infusing multiscale features."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
