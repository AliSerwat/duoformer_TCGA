{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b66acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "symbol_tracer_script = Path(\"/teamspace/studios/this_studio/symbol_tracer.py\")\n",
    "duoformer_dir = Path(\"/teamspace/studios/this_studio/duoformer_TCGA/models\")\n",
    "current_dir = Path().resolve().parent\n",
    "for p in (str(current_dir), str(duoformer_dir), str(symbol_tracer_script.parent)):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "from symbol_tracer import SymbolTracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c4b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a4dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_containing_this_file = Path().resolve().parent\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2fb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, dir_containing_this_file)\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, num_layers=2, proj_dim=768, backbone=\"r50\"):\n",
    "        super().__init__()\n",
    "        if backbone == \"r50\":\n",
    "            if num_layers == 1:\n",
    "                self.proj_heads = nn.Conv2d(\n",
    "                    2048, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                # self.proj_heads = nn.Conv2d(512, proj_dim, kernel_size=(1,1),stride=(1,1))\n",
    "                self._initialize_weights(self.proj_heads)\n",
    "            elif num_layers == 2:\n",
    "                self.proj_heads3 = nn.Conv2d(\n",
    "                    2048, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads2 = nn.Conv2d(\n",
    "                    1024, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self._initialize_weights(self.proj_heads3)\n",
    "                self._initialize_weights(self.proj_heads2)\n",
    "            elif num_layers == 3:\n",
    "                self.proj_heads3 = nn.Conv2d(\n",
    "                    2048, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads2 = nn.Conv2d(\n",
    "                    1024, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads1 = nn.Conv2d(\n",
    "                    512, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self._initialize_weights(self.proj_heads3)\n",
    "                self._initialize_weights(self.proj_heads2)\n",
    "                self._initialize_weights(self.proj_heads1)\n",
    "            elif num_layers == 4:\n",
    "                self.proj_heads3 = nn.Conv2d(\n",
    "                    2048, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads2 = nn.Conv2d(\n",
    "                    1024, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads1 = nn.Conv2d(\n",
    "                    512, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads0 = nn.Conv2d(\n",
    "                    256, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self._initialize_weights(self.proj_heads3)\n",
    "                self._initialize_weights(self.proj_heads2)\n",
    "                self._initialize_weights(self.proj_heads1)\n",
    "                self._initialize_weights(self.proj_heads0)\n",
    "        elif backbone == \"r18\":\n",
    "            if num_layers == 1:\n",
    "                self.proj_heads = nn.Conv2d(\n",
    "                    512, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self._initialize_weights(self.proj_heads)\n",
    "            elif num_layers == 2:\n",
    "                # self.proj_heads3 = nn.Conv2d(512, proj_dim, kernel_size=(1,1),stride=(1,1))\n",
    "                self.proj_heads2 = nn.Conv2d(\n",
    "                    256, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads1 = nn.Conv2d(\n",
    "                    128, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                # self.proj_heads0 = nn.Conv2d(64, proj_dim, kernel_size=(1,1),stride=(1,1))\n",
    "                # self._initialize_weights(self.proj_heads3)\n",
    "                self._initialize_weights(self.proj_heads2)\n",
    "                self._initialize_weights(self.proj_heads1)\n",
    "                # self._initialize_weights(self.proj_heads0)\n",
    "            elif num_layers == 3:\n",
    "                # self.proj_heads3 = nn.Conv2d(512, proj_dim, kernel_size=(1,1),stride=(1,1))\n",
    "                self.proj_heads0 = nn.Conv2d(\n",
    "                    64, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads2 = nn.Conv2d(\n",
    "                    256, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads1 = nn.Conv2d(\n",
    "                    128, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                # self._initialize_weights(self.proj_heads3)\n",
    "                self._initialize_weights(self.proj_heads0)\n",
    "                self._initialize_weights(self.proj_heads2)\n",
    "                self._initialize_weights(self.proj_heads1)\n",
    "            elif num_layers == 4:\n",
    "                self.proj_heads3 = nn.Conv2d(\n",
    "                    512, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads2 = nn.Conv2d(\n",
    "                    256, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads1 = nn.Conv2d(\n",
    "                    128, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self.proj_heads0 = nn.Conv2d(\n",
    "                    64, proj_dim, kernel_size=(1, 1), stride=(1, 1)\n",
    "                )\n",
    "                self._initialize_weights(self.proj_heads3)\n",
    "                self._initialize_weights(self.proj_heads2)\n",
    "                self._initialize_weights(self.proj_heads1)\n",
    "                self._initialize_weights(self.proj_heads0)\n",
    "\n",
    "        # else:\n",
    "        #     self.proj_heads = nn.ModuleDict()\n",
    "        #     for i in range(num_layers):\n",
    "        #         # self.proj_heads[f'{i}'] = nn.Linear(256 * (2 ** i), proj_dim)\n",
    "        #         self.proj_heads[f'{3-i}'] = nn.Conv2d(256 * (2 ** (3-i)), proj_dim, kernel_size=(1,1),stride=(1,1))\n",
    "        #         self._initialize_weights(self.proj_heads[f'{i}'])\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # nn.init.xavier_uniform_(module.weight)  # nn.init.xavier_uniform_() or nn.init.xavier_normal_()\n",
    "            nn.init.kaiming_normal_(\n",
    "                module.weight\n",
    "            )  # nn.init.kaiming_uniform_ ()or nn.init.kaiming_normal_()\n",
    "            if module.bias is not None:\n",
    "                # nn.init.constant_(module.bias, 0)\n",
    "                nn.init.normal_(module.bias, std=1e-6)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                # nn.init.constant_(module.bias, 0)\n",
    "                nn.init.normal_(module.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x) != 1:\n",
    "            proj_features = {}\n",
    "            for k, fea in x.items():\n",
    "                N, C, H, W = fea.shape\n",
    "                if k == \"3\":\n",
    "                    proj_features[k] = self.proj_heads3(fea)\n",
    "                elif k == \"2\":\n",
    "                    proj_features[k] = self.proj_heads2(fea)\n",
    "                elif k == \"1\":\n",
    "                    proj_features[k] = self.proj_heads1(fea)\n",
    "                elif k == \"0\":\n",
    "                    proj_features[k] = self.proj_heads0(fea)\n",
    "        else:\n",
    "            proj_features = self.proj_heads(x)\n",
    "        return proj_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel_Projector_layer1(nn.Module):\n",
    "    def __init__(self, backbone=\"r50\"):\n",
    "        super().__init__()\n",
    "        # Convolutional layers to reduce spatial dimensions\n",
    "        if backbone == \"r50\":\n",
    "            self.conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "            self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        elif backbone == \"r18\":\n",
    "            self.conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "            self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Pooling layer to downsample to 7x7\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self._initialize_weights(self.conv1)\n",
    "        self._initialize_weights(self.conv2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.norm1(x)\n",
    "        # x = self.activation1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.norm2(x)\n",
    "        # x = self.activation2(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(\n",
    "                module.weight\n",
    "            )  # nn.init.kaiming_uniform_ ()or nn.init.kaiming_normal_()\n",
    "            if module.bias is not None:\n",
    "                nn.init.normal_(module.bias, std=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel_Projector_layer2(nn.Module):\n",
    "    def __init__(self, backbone=\"r50\"):\n",
    "        super().__init__()\n",
    "        # Convolutional layers to reduce spatial dimensions\n",
    "        if backbone == \"r50\":\n",
    "            self.conv1 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1)\n",
    "        elif backbone == \"r18\":\n",
    "            self.conv1 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self._initialize_weights(self.conv1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.norm1(x)\n",
    "        # x = self.activation1(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(\n",
    "                module.weight\n",
    "            )  # nn.init.kaiming_uniform_ ()or nn.init.kaiming_normal_()\n",
    "            if module.bias is not None:\n",
    "                nn.init.normal_(module.bias, std=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd327790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel_Projector_layer3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation_type):\n",
    "    activation_type = activation_type.lower()\n",
    "    if hasattr(nn, activation_type):\n",
    "        return getattr(nn, activation_type)()\n",
    "    else:\n",
    "        return nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f651bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_nConv(in_channels, out_channels, nb_Conv, activation=\"ReLU\"):\n",
    "    layers = []\n",
    "    layers.append(ConvBatchNorm(in_channels, out_channels, activation))\n",
    "\n",
    "    for _ in range(nb_Conv - 1):\n",
    "        layers.append(ConvBatchNorm(out_channels, out_channels, activation))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBatchNorm(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU)\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, activation=\"ReLU\"):\n",
    "        super(ConvBatchNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.norm(out)\n",
    "        return self.activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fa5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel_Projector_All(nn.Module):\n",
    "    def __init__(self, backbone=\"r50\", *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if backbone == \"r50\":\n",
    "            self.nConvs = _make_nConv(3840, 768, 4)\n",
    "        elif backbone == \"r18\":\n",
    "            self.nConvs = _make_nConv(\n",
    "                384, 768, 4\n",
    "            )  # self.nConvs = _make_nConv(960, 768, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(self.nConvs(x), start_dim=2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
