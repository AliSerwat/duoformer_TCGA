{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4f856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "symbol_tracer_script = Path(\"/teamspace/studios/this_studio/symbol_tracer.py\")\n",
    "duoformer_dir = Path(\"/teamspace/studios/this_studio/duoformer_TCGA/models\")\n",
    "current_dir = Path().resolve().parent\n",
    "for p in (str(current_dir), str(duoformer_dir), str(symbol_tracer_script.parent)):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "from symbol_tracer import SymbolTracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cdde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8109d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_containing_this_file = Path().resolve().parent\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c6e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, dir_containing_this_file)\n",
    "from torchvision.models import ResNet50_Weights, ResNet18_Weights\n",
    "from backbone import *\n",
    "from scale_attention import *\n",
    "from projection_head import *\n",
    "from multiscale_attn import *\n",
    "from multi_vision_transformer import *\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.layers import Mlp, DropPath\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "import timm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb1e007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;2;255;215;0mSymbol Summary:\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;2;255;215;0mResolution Path:\u001b[0m\n",
      "  \u001b[90mâ†³\u001b[0m \u001b[38;2;30;144;255mChannel_Projector_layer1\u001b[0m \u001b[38;2;147;112;219m<type>         \u001b[0m at \u001b[38;2;0;255;255m/teamspace/studios/this_studio/duoformer_TCGA/models/projection_head.py\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[38;2;255;215;0mFinal Target Details:\u001b[0m\n",
      "  - \u001b[38;2;255;165;0mFull Name:\u001b[0m \u001b[1mChannel_Projector_layer1\u001b[0m\n",
      "  - \u001b[38;2;255;165;0mIn Module:\u001b[0m projection_head\n",
      "  - \u001b[38;2;255;165;0mFile:\u001b[0m      \u001b[38;2;0;255;255m/teamspace/studios/this_studio/duoformer_TCGA/models/projection_head.py\u001b[0m\n",
      "  - \u001b[38;2;255;165;0mSource:\u001b[0m\n",
      "\u001b[38;2;255;20;147m---\n",
      "\u001b[90mclass Channel_Projector_layer1(nn.Module):\n",
      "    def __init__(self, backbone=\"r50\"):\n",
      "        super().__init__()\n",
      "        # Convolutional layers to reduce spatial dimensions\n",
      "        if backbone == \"r50\":\n",
      "            self.conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
      "            self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
      "        elif backbone == \"r18\":\n",
      "            self.conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
      "            self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
      "\n",
      "        # Pooling layer to downsample to 7x7\n",
      "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self._initialize_weights(self.conv1)\n",
      "        self._initialize_weights(self.conv2)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.conv1(x)\n",
      "        # x = self.norm1(x)\n",
      "        # x = self.activation1(x)\n",
      "        x = self.conv2(x)\n",
      "        # x = self.norm2(x)\n",
      "        # x = self.activation2(x)\n",
      "        x = self.pool(x)\n",
      "        return x\n",
      "\n",
      "    def _initialize_weights(self, module):\n",
      "        if isinstance(module, nn.Conv2d):\n",
      "            nn.init.kaiming_normal_(\n",
      "                module.weight\n",
      "            )  # nn.init.kaiming_uniform_ ()or nn.init.kaiming_normal_()\n",
      "            if module.bias is not None:\n",
      "                nn.init.normal_(module.bias, std=1e-6)\u001b[38;2;255;20;147m\n",
      "---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(SymbolTracer(Channel_Projector_layer1).get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1660c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth=None,\n",
    "        patch_size=49,\n",
    "        embed_dim=256,\n",
    "        num_heads=6,\n",
    "        init_values=1e-5,\n",
    "        num_classes=2,\n",
    "        num_layers=4,\n",
    "        proj_dim=512,\n",
    "        model_ver=\"originalViT\",\n",
    "        pretrained=True,\n",
    "        freeze=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.name = model_ver\n",
    "        self.num_layers = num_layers\n",
    "        self.proj_dim = proj_dim\n",
    "        if pretrained:\n",
    "            self.resnet_projector = nn.Sequential(\n",
    "                *list(models.resnet50(weights=ResNet50_Weights.DEFAULT).children())[:-2]\n",
    "            )\n",
    "            print(\"resnet 50 pretrained weights loaded!\")\n",
    "            # self.resnet_projector = nn.Sequential(*list(models.resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2])\n",
    "            # self.vanilla_hybrid = timm.create_model('vit_small_r26_s32_224.augreg_in21k_ft_in1k',pretrained=True,\n",
    "            # num_classes=num_classes)\n",
    "            # self.projection = nn.Conv2d(1024, self.proj_dim, kernel_size=(1,1),stride=(1,1))\n",
    "            # self.projection1 = nn.Conv2d(2048, 768, kernel_size=(1,1),stride=(1,1))\n",
    "            # nn.init.kaiming_normal_(self.projection.weight)\n",
    "            # if self.projection.bias is not None:\n",
    "            #     nn.init.normal_(self.projection.bias, std=.02)\n",
    "            # nn.init.kaiming_normal_(self.projection1.weight)\n",
    "            # if self.projection1.bias is not None:\n",
    "            #     nn.init.normal_(self.projection1.bias, std=.02) # 1e-6\n",
    "            # self.channel_squeeze =  nn.Sequential(\n",
    "            #     nn.Linear(self.proj_dim, 384, bias=True),\n",
    "            #     nn.GELU(),\n",
    "            #     nn.Linear(384, 384, bias=True),\n",
    "            # )\n",
    "            # for layer in self.channel_squeeze:\n",
    "            #     if isinstance(layer, nn.Linear):\n",
    "            #         nn.init.trunc_normal_(layer.weight, std=0.02)\n",
    "            #         nn.init.zeros_(layer.bias)\n",
    "\n",
    "        else:\n",
    "            self.resnet_projector = nn.Sequential(\n",
    "                *list(models.resnet50().children())[:-2]\n",
    "            )\n",
    "            # self.resnet_projector = nn.Sequential(*list(models.resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2])\n",
    "            # print(\"resnet 18 pretrained weights loaded!\")\n",
    "            # self.resnet_projector = Backbone()\n",
    "            # self.resnet_projector2 = Backbone2()\n",
    "\n",
    "        self.chann_proj1 = Channel_Projector_layer1()\n",
    "        self.chann_proj2 = Channel_Projector_layer2()\n",
    "        self.chann_proj3 = Channel_Projector_layer3()\n",
    "        self.chann_proj_all = Channel_Projector_All()\n",
    "        self.projection = Projection(\n",
    "            num_layers=self.num_layers, proj_dim=self.proj_dim, backbone=\"r50\"\n",
    "        )\n",
    "        if self.num_layers > 1:\n",
    "            self.vision_transformer = MultiscaleTransformer(\n",
    "                pretrained=pretrained,\n",
    "                depth=depth,\n",
    "                scales=num_layers,\n",
    "                num_heads=num_heads,\n",
    "                patch_size=patch_size,\n",
    "                embed_dim=embed_dim,\n",
    "                init_values=init_values,\n",
    "                num_classes=num_classes,\n",
    "                model_type=self.name,\n",
    "                attn_drop_rate=0.1,\n",
    "                drop_rate=0.1,\n",
    "            )\n",
    "            print(\"multiscaletransformer!\")\n",
    "            # self.scale_former = ScaleFormer(depth=12,scales=self.num_layers,num_heads=12,embed_dim=self.proj_dim) # consistent with pretrained hybrid\n",
    "\n",
    "        # self.vanilla_vit= timm.models.vision_transformer.vit_base_patch32_224(pretrained=True,num_classes = num_classes)\n",
    "        if freeze == True:\n",
    "            for param in self.resnet_projector.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Backbone freezed!\")\n",
    "\n",
    "        self.index = {}\n",
    "        for i in range(4):\n",
    "            self.index[f\"{4-i-1}\"] = torch.empty([49, 4**i], dtype=torch.int32)\n",
    "\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                p = r * 7 + c\n",
    "                self.index[\"3\"][p, :] = p\n",
    "                self.index[\"2\"][p, :] = torch.IntTensor(\n",
    "                    [\n",
    "                        2 * r * 14 + 2 * c,\n",
    "                        (2 * r + 1) * 14 + 2 * c,\n",
    "                        2 * r * 14 + (2 * c + 1),\n",
    "                        (2 * r + 1) * 14 + (2 * c + 1),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                self.index[\"1\"][p, :] = torch.IntTensor(\n",
    "                    [\n",
    "                        4 * r * 28 + 4 * c,\n",
    "                        4 * r * 28 + 4 * c + 1,\n",
    "                        4 * r * 28 + 4 * c + 2,\n",
    "                        4 * r * 28 + 4 * c + 3,\n",
    "                        (4 * r + 1) * 28 + 4 * c,\n",
    "                        (4 * r + 1) * 28 + 4 * c + 1,\n",
    "                        (4 * r + 1) * 28 + 4 * c + 2,\n",
    "                        (4 * r + 1) * 28 + 4 * c + 3,\n",
    "                        (4 * r + 2) * 28 + 4 * c,\n",
    "                        (4 * r + 2) * 28 + 4 * c + 1,\n",
    "                        (4 * r + 2) * 28 + 4 * c + 2,\n",
    "                        (4 * r + 2) * 28 + 4 * c + 3,\n",
    "                        (4 * r + 3) * 28 + 4 * c,\n",
    "                        (4 * r + 3) * 28 + 4 * c + 1,\n",
    "                        (4 * r + 3) * 28 + 4 * c + 2,\n",
    "                        (4 * r + 3) * 28 + 4 * c + 3,\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                self.index[\"0\"][p, :] = torch.IntTensor(\n",
    "                    [\n",
    "                        8 * r * 56 + 8 * c,\n",
    "                        8 * r * 56 + 8 * c + 1,\n",
    "                        8 * r * 56 + 8 * c + 2,\n",
    "                        8 * r * 56 + 8 * c + 3,\n",
    "                        8 * r * 56 + 8 * c + 4,\n",
    "                        8 * r * 56 + 8 * c + 5,\n",
    "                        8 * r * 56 + 8 * c + 6,\n",
    "                        8 * r * 56 + 8 * c + 7,\n",
    "                        (8 * r + 1) * 56 + 8 * c,\n",
    "                        (8 * r + 1) * 56 + 8 * c + 1,\n",
    "                        (8 * r + 1) * 56 + 8 * c + 2,\n",
    "                        (8 * r + 1) * 56 + 8 * c + 3,\n",
    "                        (8 * r + 1) * 56 + 8 * c + 4,\n",
    "                        (8 * r + 1) * 56 + 8 * c + 5,\n",
    "                        (8 * r + 1) * 56 + 8 * c + 6,\n",
    "                        (8 * r + 1) * 56 + 8 * c + 7,\n",
    "                        (8 * r + 2) * 56 + 8 * c,\n",
    "                        (8 * r + 2) * 56 + 8 * c + 1,\n",
    "                        (8 * r + 2) * 56 + 8 * c + 2,\n",
    "                        (8 * r + 2) * 56 + 8 * c + 3,\n",
    "                        (8 * r + 2) * 56 + 8 * c + 4,\n",
    "                        (8 * r + 2) * 56 + 8 * c + 5,\n",
    "                        (8 * r + 2) * 56 + 8 * c + 6,\n",
    "                        (8 * r + 2) * 56 + 8 * c + 7,\n",
    "                        (8 * r + 3) * 56 + 8 * c,\n",
    "                        (8 * r + 3) * 56 + 8 * c + 1,\n",
    "                        (8 * r + 3) * 56 + 8 * c + 2,\n",
    "                        (8 * r + 3) * 56 + 8 * c + 3,\n",
    "                        (8 * r + 3) * 56 + 8 * c + 4,\n",
    "                        (8 * r + 3) * 56 + 8 * c + 5,\n",
    "                        (8 * r + 3) * 56 + 8 * c + 6,\n",
    "                        (8 * r + 3) * 56 + 8 * c + 7,\n",
    "                        (8 * r + 4) * 56 + 8 * c,\n",
    "                        (8 * r + 4) * 56 + 8 * c + 1,\n",
    "                        (8 * r + 4) * 56 + 8 * c + 2,\n",
    "                        (8 * r + 4) * 56 + 8 * c + 3,\n",
    "                        (8 * r + 4) * 56 + 8 * c + 4,\n",
    "                        (8 * r + 4) * 56 + 8 * c + 5,\n",
    "                        (8 * r + 4) * 56 + 8 * c + 6,\n",
    "                        (8 * r + 4) * 56 + 8 * c + 7,\n",
    "                        (8 * r + 5) * 56 + 8 * c,\n",
    "                        (8 * r + 5) * 56 + 8 * c + 1,\n",
    "                        (8 * r + 5) * 56 + 8 * c + 2,\n",
    "                        (8 * r + 5) * 56 + 8 * c + 3,\n",
    "                        (8 * r + 5) * 56 + 8 * c + 4,\n",
    "                        (8 * r + 5) * 56 + 8 * c + 5,\n",
    "                        (8 * r + 5) * 56 + 8 * c + 6,\n",
    "                        (8 * r + 5) * 56 + 8 * c + 7,\n",
    "                        (8 * r + 6) * 56 + 8 * c,\n",
    "                        (8 * r + 6) * 56 + 8 * c + 1,\n",
    "                        (8 * r + 6) * 56 + 8 * c + 2,\n",
    "                        (8 * r + 6) * 56 + 8 * c + 3,\n",
    "                        (8 * r + 6) * 56 + 8 * c + 4,\n",
    "                        (8 * r + 6) * 56 + 8 * c + 5,\n",
    "                        (8 * r + 6) * 56 + 8 * c + 6,\n",
    "                        (8 * r + 6) * 56 + 8 * c + 7,\n",
    "                        (8 * r + 7) * 56 + 8 * c,\n",
    "                        (8 * r + 7) * 56 + 8 * c + 1,\n",
    "                        (8 * r + 7) * 56 + 8 * c + 2,\n",
    "                        (8 * r + 7) * 56 + 8 * c + 3,\n",
    "                        (8 * r + 7) * 56 + 8 * c + 4,\n",
    "                        (8 * r + 7) * 56 + 8 * c + 5,\n",
    "                        (8 * r + 7) * 56 + 8 * c + 6,\n",
    "                        (8 * r + 7) * 56 + 8 * c + 7,\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "    def get_features(self, x):\n",
    "        layers = []\n",
    "        for i in range(4):  # self.num_layers\n",
    "            layers.append(str(7 - i))\n",
    "        # layers = ['4','5'] # '5','4'\n",
    "        features = {}\n",
    "        for name, module in list(self.resnet_projector.named_children()):\n",
    "            x = module(x)\n",
    "            if name in layers:\n",
    "                features[str(int(name) - 4)] = x\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.get_features(x)\n",
    "        # x0 = self.resnet_projector(x) # x1,x2,x3\n",
    "        # x1 = self.resnet_projector2(x)# x1,x2\n",
    "        ################ for pretrained hybrid model ###############\n",
    "        # x = self.vanilla_hybrid.patch_embed.backbone.stem(x) # 64, 56, 56\n",
    "        # x1 = self.vanilla_hybrid.patch_embed.backbone.stages[0](x) # 256, 56, 56\n",
    "        # x2 = self.vanilla_hybrid.patch_embed.backbone.stages[1](x1) # 512, 28, 28\n",
    "        # x3 = self.vanilla_hybrid.patch_embed.backbone.stages[2](x2) # 1024, 14, 14\n",
    "        # x4 = self.vanilla_hybrid.patch_embed.backbone.stages[3](x3) # 2048, 7, 7\n",
    "        # x4 = self.vanilla_hybrid.patch_embed.backbone.norm(x4)\n",
    "        # x4 = self.vanilla_hybrid.patch_embed.backbone.head(x4, pre_logits=False) # All Identity by default\n",
    "        # # output = self.vanilla_hybrid.patch_embed.proj(x4) # 384, 7, 7 # use pretrained patch emb\n",
    "        # x4 = self.projection1(x4)\n",
    "        # x3 = self.projection(x3) # this is from scratch\n",
    "        # # x2 = self.projection1(x2) # this is from scratch\n",
    "\n",
    "        # B,C,H,W = x4.shape\n",
    "        # x4 = x4.reshape(B,C,-1) # B, 384, 49\n",
    "        # x3 = x3.reshape(B,C,-1) # B, 384, 196\n",
    "        # x4= x4[:,:,self.index['3']]\n",
    "        # x3= x3[:,:,self.index['2']]\n",
    "        # # # x2 = x2.reshape(B,C,-1) # B, 384, 784\n",
    "        # # # x2 = x2[:,:,self.index['1']]\n",
    "        # if self.num_layers == 2:\n",
    "        #     x = torch.cat((x4,x3), dim = -1).permute(0,2,3,1) # [64, 384, 49, 5] -> [64, 49, 5, 384]\n",
    "        # # if self.num_layers == 3:\n",
    "        # #     x = torch.cat((x4,x3,x2), dim = -1).permute(0,2,3,1) # [64, 384, 49, 22] -> [64, 49, 22, 384]\n",
    "\n",
    "        # output = self.scale_former(x) # scale attention from scratch # [64, 49, 384]\n",
    "        # output = self.channel_squeeze(output)\n",
    "        # # x4_ori = x4_ori.reshape(x4_ori.shape[0],384,-1).permute(0,2,1)\n",
    "        # # output = output * x4_ori\n",
    "        # # print(output.shape)\n",
    "        # # output = output.reshape[output.shape[0],self.proj_dim,7,7]\n",
    "        # # print(output.shape)\n",
    "        # # output = self.vanilla_hybrid.patch_embed.proj(output)\n",
    "        # # patch attention from pretrained\n",
    "        # # cls_token = self.vanilla_hybrid.cls_token.expand(output.shape[0], -1, -1)\n",
    "        # # output = torch.cat((cls_token, output), dim=1)\n",
    "        # # output = output + self.vanilla_hybrid.pos_embed\n",
    "        # # output = self.vanilla_hybrid.pos_drop(output)\n",
    "        # # output = output.reshape(output.shape[0],output.shape[1],-1).permute(0,2,1)\n",
    "        # output = self.vanilla_hybrid._pos_embed(output)\n",
    "        # output = self.vanilla_hybrid.norm_pre(output)\n",
    "        # output = self.vanilla_hybrid.blocks(output)\n",
    "        # output = self.vanilla_hybrid.norm(output)\n",
    "        # output = self.vanilla_hybrid.forward_head(output)\n",
    "        # output = self.vanilla_hybrid(x)\n",
    "        ################ end for pretrained hybrid model ###############\n",
    "        # fea = {'3':x0[-1],'2':x1[-1]} # using features from different encoder\n",
    "        # fea['3'] = self.projection.proj_heads3(x[-1])\n",
    "        # fea['2'] = self.projection.proj_heads2(x[-2])\n",
    "        # channel\n",
    "        channel_fuse = {}\n",
    "        channel_fuse[\"0\"] = self.chann_proj1(x[\"0\"])\n",
    "        channel_fuse[\"1\"] = self.chann_proj2(x[\"1\"])\n",
    "        channel_fuse[\"2\"] = self.chann_proj3(x[\"2\"])\n",
    "        channel_fuse[\"3\"] = x[\"3\"]\n",
    "        channel_fuse_all = torch.cat(\n",
    "            [channel_fuse[key] for key in sorted(channel_fuse.keys())], dim=1\n",
    "        )\n",
    "        channel_token = (\n",
    "            self.chann_proj_all(channel_fuse_all).unsqueeze(-1).permute(0, 2, 3, 1)\n",
    "        )  # 49,1,768\n",
    "\n",
    "        x = self.projection({\"2\": x[\"2\"], \"3\": x[\"3\"]})\n",
    "        if self.name == \"scaleformer\":\n",
    "            # B,C,H,W = x['0'].shape\n",
    "            # x['1'] = x['1'].reshape(B,C,-1)\n",
    "            # x['0'] = x['0'].reshape(B,C,-1)\n",
    "            # x['1']= x['1'][:,:,self.index['1']]\n",
    "            # x['0']= x['0'][:,:,self.index['0']]\n",
    "            # x = torch.cat((x['1'],x['0']), dim = -1).permute(0,2,3,1)\n",
    "            # x = x['0'].permute(0,2,3,1)\n",
    "            B, C, H, W = x[\"3\"].shape\n",
    "            x[\"3\"] = x[\"3\"].reshape(B, C, -1)\n",
    "            x[\"2\"] = x[\"2\"].reshape(B, C, -1)\n",
    "            # [64, 768, 7, 7] -> [64, 49, 1, 7, 7]\n",
    "            x[\"3\"] = x[\"3\"][:, :, self.index[\"3\"]]\n",
    "            # [64, 768, 14, 14] -> [64, 49, 4, 14, 14]\n",
    "            x[\"2\"] = x[\"2\"][:, :, self.index[\"2\"]]\n",
    "            if self.num_layers == 2:\n",
    "                # [64, 768, 49, 5] -> [64, 49, 5, 768]\n",
    "                x = torch.cat((x[\"3\"], x[\"2\"]), dim=-1).permute(0, 2, 3, 1)\n",
    "            elif self.num_layers == 4:\n",
    "                x[\"1\"] = x[\"1\"].reshape(B, C, -1)\n",
    "                x[\"0\"] = x[\"0\"].reshape(B, C, -1)\n",
    "                x[\"1\"] = x[\"1\"][:, :, self.index[\"1\"]]\n",
    "                x[\"0\"] = x[\"0\"][:, :, self.index[\"0\"]]\n",
    "                x = torch.cat((x[\"3\"], x[\"2\"], x[\"1\"], x[\"0\"]), dim=-1).permute(\n",
    "                    0, 2, 3, 1\n",
    "                )\n",
    "            elif self.num_layers == 3:\n",
    "                x[\"1\"] = x[\"1\"].reshape(B, C, -1)\n",
    "                x[\"1\"] = x[\"1\"][:, :, self.index[\"1\"]]\n",
    "                x = torch.cat((x[\"3\"], x[\"2\"], x[\"1\"]), dim=-1).permute(0, 2, 3, 1)\n",
    "        x = torch.cat((channel_token, x), dim=2)\n",
    "        output = self.vision_transformer(x)\n",
    "        # cls_token = self.vanilla_vit.cls_token.expand(output.shape[0], -1, -1)\n",
    "        # output = torch.cat((cls_token, output), dim=1)\n",
    "        # output = output + self.vanilla_vit.pos_embed\n",
    "        # output = self.vanilla_vit.pos_drop(output)\n",
    "        # output = self.vanilla_vit.norm_pre(output)\n",
    "        # output = self.vanilla_vit.blocks(output)\n",
    "        # output = self.vanilla_vit.norm(output)\n",
    "        # output = self.vanilla_vit.forward_head(output)\n",
    "        # x = x['3'].permute(0,2,1)\n",
    "        # x = self.vanilla_vit._pos_embed(x)\n",
    "        # x = self.vanilla_vit.patch_drop(x)\n",
    "        # x = self.vanilla_vit.norm_pre(x)\n",
    "        # x = self.vanilla_vit.blocks(x)\n",
    "        # x = self.vanilla_vit.norm(x)\n",
    "        # x = self.vanilla_vit.forward_head(x)\n",
    "        # return x\n",
    "        # x1,x2,x3,output = self.resnet_projector(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, num_classes=100, num_blocks=12, proj_dim=768, num_heads=6):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.proj_dim = proj_dim\n",
    "        self.resnet_projector = nn.Sequential(\n",
    "            *list(models.resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2]\n",
    "        )\n",
    "        # for param in self.resnet_projector.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # for baseline 2\n",
    "        self.projection = Projection(num_layers=1, proj_dim=self.proj_dim)\n",
    "        self.vision_transformer = VisionTransformer(\n",
    "            patch_size=32, depth=self.num_blocks, num_classes=num_classes\n",
    "        )  # n_patches:49\n",
    "        # for mixed feature experiment, we only need the vanilla ViT without patch embedding\n",
    "        # self.vision_transformer = VisionTransformer(num_classes=num_classes,depth=self.num_blocks,embed_dim=proj_dim)  # emb_dim: 512, n_patches:196, attn_dim:196\n",
    "        # for mixed feature experiment wi two attn, emb_dim: 512, n_patches:196, attn_dim:512\n",
    "        # self.projection1 = Projection(num_layers=1, proj_dim=784*4 )\n",
    "        # self.projection2 = nn.Conv2d(784, proj_dim, kernel_size=(1,1),stride=(1,1))\n",
    "        # nn.Conv1d(in_channels=512, out_channels=proj_dim, kernel_size=1)\n",
    "        # nn.init.kaiming_normal_(self.projection2.weight)\n",
    "        # if self.projection2.bias is not None:\n",
    "        # nn.init.normal_(self.projection2.bias, std=1e-6)\n",
    "        # self.vision_transformer2 = VisionTransformer(num_classes=num_classes,depth=self.num_blocks,patch_size=8,embed_dim=196,num_heads=num_heads,class_token=False,global_pool = '') # 224/8 x 224/8 =28x28=784\n",
    "        # self.test =  nn.Linear(784,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet_projector(x)  # 2048,7,7\n",
    "        x = self.projection(x)  # 784*4,7,7\n",
    "        # this is for baseline 2: a pretrained r50 + vit from scratch(replacing patch emb by a single layer projection)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        # remove patch emb layer from ViT\n",
    "        x = self.vision_transformer._pos_embed(x)\n",
    "        x = self.vision_transformer.patch_drop(x)\n",
    "        x = self.vision_transformer.norm_pre(x)\n",
    "        x = self.vision_transformer.blocks(x)\n",
    "        x = self.vision_transformer.norm(x)\n",
    "        x = self.vision_transformer.forward_head(x)\n",
    "        # end of baseline 2\n",
    "\n",
    "        # this is for mixed feature experiment\n",
    "        # B,C,H,W = x.shape\n",
    "        # x = x.reshape(B,784,-1) # transpose(1, 2)\n",
    "\n",
    "        # ### for mixed feature with two attns\n",
    "        # B,C,H,W = x.shape  # 784*4,7,7\n",
    "        # x = x.reshape(B,784,-1) # permute(0,2, 1) # 784,196\n",
    "        # x = self.vision_transformer2._pos_embed(x)\n",
    "        # x = self.vision_transformer2.patch_drop(x)\n",
    "        # x = self.vision_transformer2.norm_pre(x)\n",
    "        # x = self.vision_transformer2.blocks(x)\n",
    "        # x = self.vision_transformer2.norm(x)\n",
    "        # # x = x[:,:,0]\n",
    "        # # x = self.test(x)\n",
    "        # x = x.reshape(B,784,2*H,2*W) # 784,14,14\n",
    "        # x = self.projection2(x) # 768,14,14\n",
    "        # x = x.reshape(B,self.proj_dim,-1).permute(0,2, 1) # 196,768\n",
    "\n",
    "        # remove patch emb layer from ViT\n",
    "        # x = self.vision_transformer._pos_embed(x)\n",
    "        # x = self.vision_transformer.patch_drop(x)\n",
    "        # x = self.vision_transformer.norm_pre(x)\n",
    "        # x = self.vision_transformer.blocks(x)\n",
    "        # x = self.vision_transformer.norm(x)\n",
    "        # # # # x = x[:,0,:]\n",
    "        # # # # x = self.test(x)\n",
    "        # x = self.vision_transformer.forward_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5245d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBase16(nn.Module):\n",
    "    def __init__(self, n_classes=100, model_type=\"R50ViT\"):\n",
    "        super().__init__()\n",
    "        if model_type == \"ViT\":\n",
    "            self.model = VisionTransformer(num_classes=n_classes)\n",
    "\n",
    "        elif model_type == \"ViTPretrained\":\n",
    "            self.model = timm.create_model(\n",
    "                \"vit_base_r50_s16_224_in21k\", pretrained=True, num_classes=n_classes\n",
    "            )\n",
    "\n",
    "        elif model_type == \"R50ViTPretrained\":\n",
    "            self.model = timm.create_model(\n",
    "                \"vit_base_r50_s16_224_in21k\", pretrained=True, num_classes=n_classes\n",
    "            )\n",
    "            print(model_type, \"is created.\")\n",
    "\n",
    "        elif model_type == \"R50ViT\":\n",
    "            # self.model = timm.create_model(\n",
    "            # 'vit_base_r50_s16_224_in21k',pretrained=False,\n",
    "            # num_classes=n_classes)\n",
    "            self.model = timm.create_model(\n",
    "                \"vit_small_r26_s32_224.augreg_in21k_ft_in1k\",\n",
    "                pretrained=True,\n",
    "                num_classes=n_classes,\n",
    "            )\n",
    "            print(\"pretrained hybrid loaded!\")\n",
    "        # self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
    "        self.name = model_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return trainable_params / 1000000, total_params / 1000000"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
