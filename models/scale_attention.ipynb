{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_containing_this_file = Path(__file__).resolve().parent\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, dir_containing_this_file)\n",
    "import torch\n",
    "from timm.models.vision_transformer import VisionTransformer, LayerScale\n",
    "from timm.layers import Mlp, DropPath\n",
    "from timm.models.vision_transformer import init_weights_vit_timm, get_init_weights_vit\n",
    "from timm.models._manipulate import named_apply\n",
    "from timm.layers import trunc_normal_\n",
    "import timm\n",
    "from multiscale_attn import *\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from typing import Callable, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ab13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scale Attention blocks\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd615b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionForScale(Attention):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0, proj_drop=0):\n",
    "        super().__init__(dim, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "        # self.scale = 2 * dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, num_regions, num_scales, C = x.shape  # N, 49, 86, d\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, num_regions, num_scales, 3, self.num_heads, self.head_dim)\n",
    "            .permute(3, 0, 1, 4, 2, 5)\n",
    "        )  # [3,B,49,6,N(86),d/6] 20314\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # [B,49,6,N(86),d/86]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B,49,6,86,86]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(2, 3).reshape(B, num_regions, num_scales, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_norm=False,\n",
    "        proj_drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        init_values=None,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        mlp_layer=Mlp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = AttentionForScale(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "        )\n",
    "        self.ls1 = (\n",
    "            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        )\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.ls2 = (\n",
    "            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        )\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bde50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleFormer(nn.Module):\n",
    "    \"\"\"Only scale attention blocks was included in ScaleFormer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth=12,\n",
    "        scales=2,\n",
    "        num_heads=6,\n",
    "        embed_dim=384,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_norm=False,\n",
    "        proj_drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        norm_layer=None,\n",
    "        act_layer=None,\n",
    "        init_values=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        pos_drop_rate = 0.0\n",
    "        patch_drop_rate = 0.0  # patch dropout in vision transformer's implementation. Here we do not use it for scale attention.\n",
    "        self.norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.act_layer = act_layer or nn.GELU\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                ScaleBlock(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_norm=qk_norm,\n",
    "                    init_values=init_values,\n",
    "                    proj_drop=proj_drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    norm_layer=self.norm_layer,\n",
    "                    act_layer=self.act_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.cls_token1 = nn.Parameter(\n",
    "            torch.randn(1, 1, 1, embed_dim)\n",
    "        )  # 1,49,1,embed_dim not good as the default\n",
    "        if scales == 2:\n",
    "            self.fea_dim = 6  # 6\n",
    "        elif scales == 3:\n",
    "            self.fea_dim = 22\n",
    "        elif scales == 4:\n",
    "            self.fea_dim = 86  # 2,6,22,86\n",
    "\n",
    "        self.pos_embed_for_scale = nn.Parameter(\n",
    "            torch.randn(1, 1, self.fea_dim, embed_dim)\n",
    "        )  # # 1,49,self.fea_dim,embed_dim,embed_dim not good as the default\n",
    "        self.pos_drop_for_scale = nn.Dropout(p=pos_drop_rate)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        trunc_normal_(self.pos_embed_for_scale, std=0.036)  # 0.036,0.02\n",
    "        nn.init.normal_(self.cls_token1, std=0.036)  # 0.036,1e-6\n",
    "        named_apply(get_init_weights_vit(mode=\"\"), self.blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat(\n",
    "            (self.cls_token1.expand(x.shape[0], 49, -1, -1), x), dim=2\n",
    "        )  #  N,49,1+85,d\n",
    "        x = self.pos_drop_for_scale(x + self.pos_embed_for_scale)\n",
    "\n",
    "        for i in range(0, len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "        cls_token = x[:, :, 1, :]\n",
    "\n",
    "        return cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Patch Attention blocks\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba10d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionForPatch(Attention):\n",
    "    def __init__(self, dim=768, num_heads=8, qkv_bias=False, attn_drop=0, proj_drop=0):\n",
    "        super().__init__(dim, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "\n",
    "    def forward(self, x, cls_token=None, pos_embed=None, pos_drop=None):\n",
    "        B = x.shape[0]  # N, 49, 86, d\n",
    "        if len(x.size()) > 3:\n",
    "            scaled_x = x[\n",
    "                :, :, 0, :\n",
    "            ]  # after scale attns, use the first token after scale attention as patch token\n",
    "        else:\n",
    "            scaled_x = x\n",
    "\n",
    "        if cls_token is not None:\n",
    "            scaled_x = torch.cat((cls_token.squeeze(1), scaled_x), dim=1)  # N,CLS+49,d\n",
    "\n",
    "        if pos_embed is not None:\n",
    "            scaled_x = pos_drop(scaled_x + pos_embed)  # N,CLS+49,d\n",
    "\n",
    "        qkv1 = (\n",
    "            self.qkv(scaled_x)\n",
    "            .reshape(B, 50, 3, self.num_heads, self.head_dim)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q1, k1, v1 = qkv1[0], qkv1[1], qkv1[2]  #  N, N_H, 50, d/N_H\n",
    "        q1, k1 = self.q_norm(q1), self.k_norm(k1)  # identity in default\n",
    "\n",
    "        attn1 = (q1 @ k1.transpose(-2, -1)) * self.scale\n",
    "        attn1 = attn1.softmax(dim=-1)\n",
    "        attn1 = self.attn_drop(attn1)\n",
    "\n",
    "        scaled_x = (attn1 @ v1).transpose(1, 2).reshape(B, 50, -1)\n",
    "        scaled_x = self.proj(scaled_x)\n",
    "        scaled_x = self.proj_drop(scaled_x)  # N,50,d\n",
    "\n",
    "        return scaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchBlock(nn.Module):\n",
    "    \"\"\"Remove all Layer Scale, MLP projection and residual connection here.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        qkv_bias=False,\n",
    "        proj_drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = AttentionForPatch(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cls_token=None, pos_embed=None, pos_drop=None):\n",
    "        x = self.attn(x, cls_token=cls_token, pos_embed=pos_embed, pos_drop=pos_drop)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1650448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiscaleFormer(nn.Module):\n",
    "    \"\"\"In MultiscaleFormer class we include scale then patch attention blocks.\n",
    "    Not inheritage from vision transformer but from scratch .\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int = 12,\n",
    "        scales: int = 2,\n",
    "        num_heads: int = 12,\n",
    "        embed_dim: int = 768,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        qk_norm: bool = False,\n",
    "        proj_drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        norm_layer: Optional[Callable] = None,\n",
    "        act_layer: Optional[Callable] = None,\n",
    "        init_values: Optional[float] = None,\n",
    "        num_classes: int = 100,\n",
    "        num_patches: int = 49,\n",
    "        pos_drop_rate: float = 0.0,\n",
    "        patch_drop_rate: float = 0.0,\n",
    "        block_fn: Callable = ScaleBlock,\n",
    "        block_fn1: Callable = PatchBlock,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.act_layer = act_layer or nn.GELU\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.scaleBlocks = nn.Sequential(\n",
    "            *[\n",
    "                block_fn(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_norm=qk_norm,\n",
    "                    init_values=init_values,\n",
    "                    proj_drop=proj_drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    norm_layer=self.norm_layer,\n",
    "                    act_layer=self.act_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                block_fn1(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    proj_drop=proj_drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if scales == 2:\n",
    "            self.fea_dim = 21  # 6\n",
    "        elif scales == 3:\n",
    "            self.fea_dim = 22  # 22,82,70,85\n",
    "        elif scales == 4:\n",
    "            self.fea_dim = 86  # 2,6,22,86\n",
    "\n",
    "        embed_len = num_patches + 1  # with cls_token\n",
    "        self.pos_embed_for_scale = nn.Parameter(\n",
    "            torch.randn(1, 1, self.fea_dim, embed_dim)\n",
    "        )  # # 1,49,self.fea_dim,embed_dim,embed_dim not good as the default\n",
    "        self.pos_drop_for_scale = nn.Dropout(p=pos_drop_rate)\n",
    "\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, embed_dim)\n",
    "        )  # for patch attentions\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * 0.02)\n",
    "        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n",
    "\n",
    "        self.fc_norm = self.norm_layer(embed_dim)  # classification head\n",
    "        self.head_drop = nn.Dropout(0.0)\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        trunc_normal_(self.pos_embed_for_scale, std=0.036)  # 0.036, default: 0.02\n",
    "        trunc_normal_(self.pos_embed, std=0.036)  # 0.036, default: 0.02\n",
    "        nn.init.normal_(self.cls_token, std=0.036)  # 0.036, default: 1e-6\n",
    "        named_apply(get_init_weights_vit(mode=\"\"), self.blocks)\n",
    "        named_apply(get_init_weights_vit(mode=\"\"), self.scaleBlocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_drop_for_scale(x + self.pos_embed_for_scale)\n",
    "        for i in range(0, len(self.blocks)):  # scale attentions\n",
    "            x = self.scaleBlocks[i](x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1, -1)\n",
    "        for i in range(0, len(self.blocks)):  # patch attentions\n",
    "            if i == 0:\n",
    "                x = self.blocks[i](x, cls_token, self.pos_embed, self.pos_drop)\n",
    "            else:\n",
    "                x = self.blocks[i](x)\n",
    "        cls_token = x[:, 0, :]\n",
    "        x = self.fc_norm(cls_token)\n",
    "        cls_token = self.head_drop(cls_token)\n",
    "        return self.head(cls_token)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
