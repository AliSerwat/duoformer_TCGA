{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7daaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_containing_this_file = Path(__file__).resolve().parent\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ed778",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, dir_containing_this_file)\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import torch\n",
    "from timm.models.vision_transformer import Attention, Block, LayerScale\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from timm.layers import PatchEmbed, Mlp, DropPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54192bfc",
   "metadata": {},
   "source": [
    "version with LayerScale\n",
    "class MultiScaleAttention(Attention):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0, proj_drop=0):\n",
    "        super().__init__(dim, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "        # self.num_heads = num_heads\n",
    "        # head_dim = dim // num_heads\n",
    "        # self.scale = head_dim ** -0.5 # sqrt(dk)/2\n",
    "        self.scale = dim ** -0.5\n",
    "        self.qkv1 = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop1 = nn.Dropout(attn_drop)\n",
    "        self.proj1 = nn.Linear(dim, dim)\n",
    "        self.proj_drop1 = nn.Dropout(proj_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355a3f9",
   "metadata": {},
   "source": [
    "    def forward_with_scale(self, x):\n",
    "        B, num_regions, num_scales, C = x.shape # N, 49, 86, d\n",
    "        qkv = self.qkv1(x).reshape(B, num_regions,num_scales, 3, self.num_heads, self.head_dim).permute(3, 0, 1, 4, 2, 5) # [3,B,49,6,N(86),d/6] 20314\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # [B,49,6,N(86),d/86]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3178146",
   "metadata": {},
   "source": [
    "        attn = (q @ k.transpose(-2, -1)) * self.scale # [B,49,6,86,86]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop1(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5339b",
   "metadata": {},
   "source": [
    "        x = (attn @ v).transpose(2, 3).reshape(B, num_regions,num_scales, C)\n",
    "        x = self.proj1(x)\n",
    "        x = self.proj_drop1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f8d25",
   "metadata": {},
   "source": [
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286c214",
   "metadata": {},
   "source": [
    "    def forward_with_region(self, x,cls_token,pos_embed=None,pos_drop=None):\n",
    "        B, num_regions, num_scales, C = x.shape # N, 49, 86, d\n",
    "        scaled_x = x[:,:,0,:] # use the first token after scale attention as patch token\n",
    "        scaled_x = torch.cat((cls_token.squeeze(1), scaled_x), dim=1) # N,CLS+49,d\n",
    "        # Pos_emb from pretrained ViT\n",
    "        if pos_embed is not None:\n",
    "            scaled_x = pos_drop(scaled_x + pos_embed) # N,CLS+49,d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab3f92",
   "metadata": {},
   "source": [
    "        qkv1 = self.qkv(scaled_x).reshape(B, num_regions+1, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q1, k1, v1 = qkv1[0], qkv1[1], qkv1[2] #  N, N_H, 50, d/N_H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa597d1",
   "metadata": {},
   "source": [
    "        attn1 = (q1 @ k1.transpose(-2, -1)) * self.scale\n",
    "        attn1 = attn1.softmax(dim=-1) # all attn the same value?\n",
    "        attn1 = self.attn_drop(attn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4eab6",
   "metadata": {},
   "source": [
    "        scaled_x = (attn1 @ v1).transpose(1, 2).reshape(B, num_regions+1, -1)\n",
    "        scaled_x = self.proj(scaled_x)\n",
    "        scaled_x = self.proj_drop(scaled_x) # N,50,d\n",
    "        cls_token = scaled_x[:,:1,:].unsqueeze(1)\n",
    "        x = torch.cat((scaled_x[:,1:,:].unsqueeze(2), x[:,:,1:,:]), dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b11d0f",
   "metadata": {},
   "source": [
    "        return x, cls_token # the CLS token should carry information of different regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61492c",
   "metadata": {},
   "source": [
    "class MultiscaleBlock(Block):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4, qkv_bias=False, qk_norm=False,init_values=None,proj_drop=0, attn_drop=0, drop_path=0, norm_layer=nn.LayerNorm, act_layer=nn.GELU,):\n",
    "        super().__init__(dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,qk_norm=qk_norm,init_values=init_values, proj_drop=proj_drop,\n",
    "                         attn_drop=attn_drop, drop_path=drop_path, act_layer=act_layer, norm_layer=norm_layer)\n",
    "        # self.norm1 = norm_layer(dim)\n",
    "        self.attn = MultiScaleAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)\n",
    "        self.attnOri = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392eef12",
   "metadata": {},
   "source": [
    "        '''WI layer scale & layer norm, mlp, attn drop.'''\n",
    "        # self.norm1_for_region = norm_layer(dim)\n",
    "        # self.ls1_for_region = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        # self.drop_path1_for_region = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        # self.norm2_for_region = norm_layer(dim)\n",
    "        # self.mlp_for_region = Mlp(\n",
    "        #     in_features=dim,\n",
    "        #     hidden_features=int(dim * mlp_ratio),\n",
    "        #     act_layer=act_layer,\n",
    "        #     drop=proj_drop,\n",
    "        # )\n",
    "        # self.ls2_for_region = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        # self.drop_path2_for_region = DropPath(drop_path) if drop_path > 0. else nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba09de",
   "metadata": {},
   "source": [
    "    def forward_new_block1(self, x,cls_token,pos_embed,pos_drop):\n",
    "        '''WI layer scale & layer norm, mlp, attn drop.'''\n",
    "        # x=x + self.drop_path1(self.ls1(self.attn.forward_with_scale(self.norm1(x))))\n",
    "        # x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a425f0",
   "metadata": {},
   "source": [
    "        # x = self.norm1_for_region(x)\n",
    "        # x, cls_token = self.attn.forward_with_region(x,cls_token,pos_embed,pos_drop)\n",
    "        # x = x + self.drop_path1_for_region(self.ls1_for_region(x))\n",
    "        # cls_token = cls_token + self.drop_path1_for_region(self.ls1_for_region(cls_token))\n",
    "        # x = x + self.drop_path2_for_region(self.ls2_for_region(self.mlp_for_region(self.norm2_for_region(x))))\n",
    "        # cls_token = cls_token + self.drop_path2_for_region(self.ls2_for_region(self.mlp_for_region(self.norm2_for_region(cls_token))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed3291",
   "metadata": {},
   "source": [
    "        '''WO layer scale or layer norm, mlp, attn drop.'''\n",
    "        x = x + self.drop_path1(self.ls1(self.attn.forward_with_scale(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        x, cls_token = self.attn.forward_with_region(x,cls_token,pos_embed,pos_drop)\n",
    "        # x = x + self.drop_path1(x)\n",
    "        return x, cls_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd55818",
   "metadata": {},
   "source": [
    "    def forward_new(self, x,cls_token):\n",
    "        '''WI layer scale & layer norm, mlp, attn drop.'''\n",
    "        # x=x + self.drop_path1(self.ls1(self.attn.forward_with_scale(self.norm1(x))))\n",
    "        # x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ce740",
   "metadata": {},
   "source": [
    "        # x = self.norm1_for_region(x)\n",
    "        # x, cls_token = self.attn.forward_with_region(x,cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87f445",
   "metadata": {},
   "source": [
    "        # x = x + self.drop_path1_for_region(self.ls1_for_region(x))\n",
    "        # cls_token = cls_token + self.drop_path1_for_region(self.ls1_for_region(cls_token))\n",
    "        # x = x + self.drop_path2_for_region(self.ls2_for_region(self.mlp_for_region(self.norm2_for_region(x))))\n",
    "        # cls_token = cls_token + self.drop_path2_for_region(self.ls2_for_region(self.mlp_for_region(self.norm2_for_region(cls_token))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14378e42",
   "metadata": {},
   "source": [
    "        '''WO layer scale or layer norm, mlp, attn drop.'''\n",
    "        x = x + self.drop_path1(self.ls1(self.attn.forward_with_scale(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        x, cls_token = self.attn.forward_with_region(x,cls_token)\n",
    "        return x, cls_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249866ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "    def forward(self, x):\n",
    "        if len(x.shape) > 3:\n",
    "            x = x.view(x.shape[0],x.shape[1]*x.shape[2],-1)\n",
    "        x = x + self.drop_path1(self.ls1(self.attnOri(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleAttention(Attention):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0, proj_drop=0):\n",
    "        super().__init__(dim, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "        # self.num_heads = num_heads\n",
    "        # head_dim = dim // num_heads\n",
    "        # self.scale = head_dim ** -0.5 # default\n",
    "        # self.scale = 2* self.scale #\n",
    "        self.scale = 2 * dim**-0.5\n",
    "        # self.scale =  (head_dim ** -0.5 )/2  # sqrt(dk)/2,0.76\n",
    "        self.qkv1 = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop1 = nn.Dropout(attn_drop)\n",
    "        self.proj1 = nn.Linear(dim, dim)\n",
    "        self.proj_drop1 = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward_with_scale(self, x):\n",
    "        B, num_regions, num_scales, C = x.shape  # N, 49, 86, d\n",
    "        qkv = (\n",
    "            self.qkv1(x)\n",
    "            .reshape(B, num_regions, num_scales, 3, self.num_heads, self.head_dim)\n",
    "            .permute(3, 0, 1, 4, 2, 5)\n",
    "        )  # [3,B,49,6,N(86),d/6] 20314\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # [B,49,6,N(86),d/6]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B,49,6,86,86]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop1(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(2, 3).reshape(B, num_regions, num_scales, C)\n",
    "        x = self.proj1(x)\n",
    "        x = self.proj_drop1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # def forward_with_region(self, x,cls_token,pos_embed=None,pos_drop=None):\n",
    "    #     B, num_regions, num_scales, C = x.shape # N, 49, 86, d\n",
    "    #     scaled_x = x[:,:,0,:] # use the first token after scale attention as patch token\n",
    "    #     scaled_x = torch.cat((cls_token.squeeze(1), scaled_x), dim=1) # N,CLS+49,d\n",
    "    #     if pos_embed is not None:\n",
    "    #         scaled_x = pos_drop(scaled_x + pos_embed) # N,CLS+49,d\n",
    "\n",
    "    #     qkv1 = self.qkv(scaled_x).reshape(B, num_regions+1, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "    #     q1, k1, v1 = qkv1[0], qkv1[1], qkv1[2] #  N, N_H, 50, d/N_H\n",
    "\n",
    "    #     attn1 = (q1 @ k1.transpose(-2, -1)) * self.scale\n",
    "    #     attn1 = attn1.softmax(dim=-1) # all attn the same value?\n",
    "    #     attn1 = self.attn_drop(attn1)\n",
    "\n",
    "    #     scaled_x = (attn1 @ v1).transpose(1, 2).reshape(B, num_regions+1, -1)\n",
    "    #     scaled_x = self.proj(scaled_x)\n",
    "    #     scaled_x = self.proj_drop(scaled_x) # N,50,d\n",
    "    #     cls_token = scaled_x[:,:1,:].unsqueeze(1)\n",
    "    #     x = torch.cat((scaled_x[:,1:,:].unsqueeze(2), x[:,:,1:,:]), dim=2)\n",
    "\n",
    "    #     return x, cls_token # the CLS token should carry information of different regions\n",
    "\n",
    "    def forward_with_region(self, x, cls_token=None, pos_embed=None, pos_drop=None):\n",
    "        B = x.shape[0]  # N, 49, 86, d\n",
    "        if len(x.size()) > 3:\n",
    "            scaled_x = x[\n",
    "                :, :, 0, :\n",
    "            ]  # use the first token after scale attention as patch token\n",
    "            # scaled_x = torch.mean(x, dim=2)\n",
    "        else:\n",
    "            scaled_x = x\n",
    "        if cls_token is not None:\n",
    "            scaled_x = torch.cat((cls_token.squeeze(1), scaled_x), dim=1)  # N,CLS+49,d\n",
    "        # Pos_emb from pretrained ViT\n",
    "        if pos_embed is not None:\n",
    "            scaled_x = pos_drop(scaled_x + pos_embed)  # N,CLS+49,d\n",
    "\n",
    "        qkv1 = (\n",
    "            self.qkv(scaled_x)\n",
    "            .reshape(B, 50, 3, self.num_heads, self.head_dim)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q1, k1, v1 = qkv1[0], qkv1[1], qkv1[2]  #  N, N_H, 50, d/N_H\n",
    "\n",
    "        attn1 = (q1 @ k1.transpose(-2, -1)) * self.scale\n",
    "        attn1 = attn1.softmax(dim=-1)  # all attn the same value?\n",
    "        attn1 = self.attn_drop(attn1)\n",
    "\n",
    "        scaled_x = (attn1 @ v1).transpose(1, 2).reshape(B, 50, -1)\n",
    "        scaled_x = self.proj(scaled_x)\n",
    "        scaled_x = self.proj_drop(scaled_x)  # N,50,d\n",
    "        # cls_token = scaled_x[:,:1,:].unsqueeze(1)\n",
    "        # x = torch.cat((scaled_x[:,1:,:].unsqueeze(2), x[:,:,1:,:]), dim=2)\n",
    "        return scaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiscaleBlock(Block):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=False,\n",
    "        qk_norm=False,\n",
    "        init_values=None,\n",
    "        proj_drop=0,\n",
    "        attn_drop=0,\n",
    "        drop_path=0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        act_layer=nn.GELU,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            init_values=init_values,\n",
    "            proj_drop=proj_drop,\n",
    "            attn_drop=attn_drop,\n",
    "            drop_path=drop_path,\n",
    "            act_layer=act_layer,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        # self.norm1 = norm_layer(dim)\n",
    "        self.attn = MultiScaleAttention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "        )\n",
    "        # self.attnOri = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)\n",
    "\n",
    "    \"\"\"for mixed order, i.e., [scale - region]->[scale - region->...->[scale - region]\"\"\"\n",
    "    # def forward_new_block1(self, x,cls_token,pos_embed,pos_drop):\n",
    "    #     x=x + self.drop_path1(self.ls1(self.attn.forward_with_scale(self.norm1(x))))\n",
    "    #     x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "    #     x, cls_token = self.attn.forward_with_region(x,cls_token,pos_embed,pos_drop)\n",
    "    #     # x = x + self.drop_path1(x)\n",
    "\n",
    "    #     return x, cls_token\n",
    "\n",
    "    # def forward_new(self, x,cls_token):\n",
    "    #     x=x + self.drop_path1(self.ls1(self.attn.forward_with_scale(self.norm1(x))))\n",
    "    #     x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "    #     x, cls_token = self.attn.forward_with_region(x,cls_token)\n",
    "    #     # x = x + self.drop_path1(x)\n",
    "    #     # x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "    #     # x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "    #     return x, cls_token\n",
    "\n",
    "    \"\"\"for separate order, i.e., scale->...->scale(12 blocks) - region->...->region(12 blocks)]\"\"\"\n",
    "\n",
    "    def forward_change_order_attn1(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn.forward_with_scale(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n",
    "\n",
    "    def forward_change_order_attn2_block1(self, x, cls_token, pos_embed, pos_drop):\n",
    "        x = self.attn.forward_with_region(x, cls_token, pos_embed, pos_drop)\n",
    "        return x\n",
    "\n",
    "    def forward_change_order_attn2(self, x):\n",
    "        x = self.attn.forward_with_region(x)\n",
    "        cls_token = x[:, 0, :]\n",
    "        # cls_token = torch.mean(x, dim=1)\n",
    "        return cls_token\n",
    "\n",
    "    \"\"\"for vanilla vit\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) > 3:\n",
    "            x = x.view(x.shape[0], x.shape[1] * x.shape[2], -1)\n",
    "        x = x + self.drop_path1(self.ls1(self.attnOri(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
